<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hugo tranquilpeak theme</title>
    <link>/index.xml</link>
    <description>Recent content on Hugo tranquilpeak theme</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 03 Jun 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Classifying Handwritten Digits Using EM and PCA</title>
      <link>/2017/06/classifying-handwritten-digits-using-em-and-pca/</link>
      <pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/classifying-handwritten-digits-using-em-and-pca/</guid>
      <description>&lt;p&gt;In this post, we’ll take the Semeion Handwritten Digits data set (&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/semeion+handwritten+digit&#34; class=&#34;uri&#34;&gt;http://archive.ics.uci.edu/ml/datasets/semeion+handwritten+digit&lt;/a&gt;) and cluster the handwritten digits data using the EM algorithm with a principle components step within each maximization.&lt;/p&gt;
&lt;p&gt;First, we’ll read in the data, load the additional libraries, and create our initial data table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;mvtnorm&amp;quot;)
library(&amp;quot;data.table&amp;quot;)
# Reading data and convert to data table
setwd(&amp;quot;C:/Users/Josh/Documents/GitHub/joshuahancock.github.io/data_sets/&amp;quot;)
data &amp;lt;- fread(&amp;quot;C:/Users/Josh/Documents/GitHub/joshuahancock.github.io/data_sets/semeion.csv&amp;quot;, header = FALSE)
x &amp;lt;- data[, 1:256]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row of the data represents one handwritten digit, which were digitally scanned and stretched into a 16x16 pixel box. Each of these 256 pixels, originally in grey scale, was thresholded into a binary value that indicates ‘black’ or ‘white’ for that pixel. There are 10 additional columns (also binary), which indicate group membership. We’ll need to separate those labels into their own data table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels &amp;lt;-apply(data[, 257:266], 1, function(xx){return(which(xx == &amp;quot;1&amp;quot;) -1)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we start clustering, we need to take care of a few global variables and run our initial clustering algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# k is the number of clusters
k &amp;lt;- 10
# n is the number of observations
n &amp;lt;- nrow(x)
# d is the number of diwmensions
d &amp;lt;- ncol(x)
# q represents the number of principal components and will need to be manually changed
q &amp;lt;- 6
# x.clusters are the clusters using k means and 100 random starts
x.clusters &amp;lt;- kmeans(x, k, nstart = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have preliminary clusters, we’ll initialize our &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; matrix, which will hold the cluster membership probabilities for each observation. We then use &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; to calculate &lt;span class=&#34;math inline&#34;&gt;\(\Pi_k\)&lt;/span&gt;, the proportion of observations assigned to cluster &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mu_k\)&lt;/span&gt;, the mean of the observations within each cluster &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# n by k matrix, initialized with zeros
gamma &amp;lt;- matrix(0, n, k)
# indicate the inital custer membership with a binary label
for(i in 1:n) {gamma[i, x.clusters$cluster[i]] = 1}
# the number of members in each cluster
N &amp;lt;- colSums(gamma)
# the percentage of the data set in cluster k
pi.hat &amp;lt;- N/n
# the mean for each pixel in each cluster
# note: a matrix is required for the t() function
mu.hat &amp;lt;- (t(gamma) %*% data.matrix(x))/ N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before beginning the EM algorithm, we must initialize a covariance matrix for each pixel of each cluster. To do this, we’ll write a function to calculate the covariance for each cluster and store the result in a list of length &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma.k &amp;lt;- function(GAMMA, X, MU, X.CLUSTERS, q){
# initialize empty list to hold sigma matricies  
sigma.list &amp;lt;- list()
# normailze columns
g.norm &amp;lt;- t(t(GAMMA)/N)
# iterate over each cluster
for(i in 1:k){
# for each observation in the data set, subtract the mu
x.mu &amp;lt;- t(apply(X, 1, function(xx) xx-MU[i, ]))
# create convariance matrix
g.x.mu &amp;lt;- t(g.norm[, i] * x.mu) %*% x.mu
# eigen decomposition
x.decomp &amp;lt;- eigen(g.x.mu)
# principal components step 
if(q &amp;gt; 0){
sig.hat &amp;lt;- sum(x.decomp$values[as.integer(q + 1):as.integer(d)]) / (d - q)
decomp.vec &amp;lt;- x.decomp$vectors[, 1:q]
lambda &amp;lt;- x.decomp$values[1:q]
W &amp;lt;- decomp.vec %*% (diag(q) * sqrt(lambda - sig.hat)) %*% t(decomp.vec)
sigma.list[[i]] &amp;lt;- W %*% t(W) + sig.hat * diag(d)
}
else {
sigma.list[[i]] &amp;lt;- diag(d) * sum(x.decomp$values) / d
}
}
return(sigma.list)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve defined our function, we can initialize our &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; element:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma.hat &amp;lt;- sigma.k(gamma, x, muHat, x.clusters, q)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our basis functions and structure initialized, we can begin with the E and M steps of the process. In order to do that, we need to define a few more functions. First, we’ll update our &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; matrix with the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update.gamma &amp;lt;- function(PI, SIGMA , X, MU){
# a temp matrix to store gamma values
p.mu.sig &amp;lt;- matrix(nrow = n, ncol = k)
# iterate over clusters
for(i in 1:k){
# weight raw probabilities by pi
p.mu.sig[, i] &amp;lt;- PI[i] * dmvnorm(X, MU[i, ], SIGMA[[i]])
}
weights &amp;lt;- rowSums(p.mu.sig)
# returns normalized rows
return(p.mu.sig / weights)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need a function to calculate the log likelihood:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LL &amp;lt;- function(PI, SIGMA, X, MU){
# keeps a running track of the sum
iter.sum &amp;lt;- c(rep(0, n))
for(i in 1:k){
iter.sum &amp;lt;- iter.sum + PI[i] * dmvnorm(X, MU[i, ], SIGMA[[i]])
}
return(sum(log(iter.sum)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we need a function to calculate the AIC, so that we’ll be able to pick the proper number of principal components to use with our algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC &amp;lt;- function(LL, q){
return(-2 * LL + 2 * (d * q + 1 - q* (q - 1) / 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we’re ready to begin our algorithm. The first step, or &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; step, updates the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; matrix. The second, or &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, step calculates the new &lt;span class=&#34;math inline&#34;&gt;\(\mu_k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\pi_k\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_k\)&lt;/span&gt; based on the updated &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; matrix. At the end of each iteration the log likelihood is calculated and after a predetermined number of iterations, the process should converge. Here, we will choose 50 iterations, which we will then examine graphically.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LL.list &amp;lt;- c()
AIC.list &amp;lt;- c()
iters &amp;lt;- 0
while(iters &amp;lt; 51){
gamma &amp;lt;- update.gamma(piHat, sigma.hat, x , mu.hat)
N &amp;lt;- colSums(gamma)
pi.hat &amp;lt;- N / n
mu.hat &amp;lt;- (t(gamma) %*% as.matrix(x)) / N
sigma.hat &amp;lt;- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood &amp;lt;- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list &amp;lt;- c(LL.list, log.likelihood)
iters &amp;lt;- iters + 1
}
AIC.q &amp;lt;- AIC(tail(LL.list, 1), q)
AIC.list &amp;lt;- c(AIC.list, AIC.q)
if(q == 0){
  ll.plot &amp;lt;- as.matrix(LL.list)
} else{
  ll.plot &amp;lt;- cbind(ll.plot, LL.list)
}
AIC.table &amp;lt;- rbind(AIC.table, cbind(q, AIC.q)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some simple code to plot the log likelihood for each &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a vector of q values
q.list &amp;lt;- c(0, 2, 4, 6)
par(mfrow=c(2,2))
# plot each column of the object holding our likelihood values
for(i in 1:4){
plot(ll.plot[, i], ylab = &amp;quot;Log Likelihood&amp;quot;, xlab = &amp;quot;Iteration&amp;quot;,
main=paste(&amp;quot;q&amp;quot;, q.list[i], sep= &amp;quot; = &amp;quot;))}&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;q_plot.jpeg&#34; alt=&#34;Fig 1: convergance of the EM algorithm&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Fig 1: convergance of the EM algorithm&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we can see from the plots, the algorithm needed far fewer than 50 iterations to converge. We can also see the dramatic change in log likelihood values and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; increases. As previously mentioned, we’re using the value of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; which minimizes AIC:&lt;/p&gt;
&lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;table style=&#34;width:29%;&#34;&gt;
&lt;caption&gt;AIC value for each &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;:&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;q&lt;/th&gt;
&lt;th&gt;AIC&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;6&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;255256.93&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;4&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;291812.73&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;2&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;346328.17&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;0&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;420376.53&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We can see that &lt;span class=&#34;math inline&#34;&gt;\(q = 6\)&lt;/span&gt; minimizes the AIC, so that is the number of principal components we’ll use for our model, which we’ll now assess for accuracy. First, we’ll create a panel plot comparing the mean of each cluster to a five random draws from the distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mai = c(0.05, 0.05, 0.05, 0.05), mfrow = c(10, 6))
for(i in 1:k){
image(t(matrix(mu.hat[i, ], byrow = TRUE, 16, 16)[16:1, ]), col = topo.colors(255, alpha = 0.75), axes = FALSE)
box()
for(j in 1:5){
randomPick &amp;lt;- rmvnorm(1, mu.hat[i, ], sigma.hat[[i]])
image(t(matrix(randomPick, byrow=TRUE, 16, 16)[16:1, ]), col = topo.colors(255, alpha = 0.75), axes = FALSE)
}
}&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;mean_plot.jpeg&#34; alt=&#34;Fig 2: cluster means versus random draws&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Fig 2: cluster means versus random draws&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A visual inspection reveals quite a bit of about the clusters. The cluster means are much more discernible than random draws from the distribution. Several are the numbers are quite distinguishable, while others are less defined, especially in pixels that intersect with similar numbers. Zero appears twice, and numbers two and three have gotten mixed in with other digits.&lt;/p&gt;
&lt;p&gt;The next step will be to examine the classification and misclassification rates of our model. We must first define one of these rates, as the other is simply &lt;span class=&#34;math inline&#34;&gt;\((1 - rate)\)&lt;/span&gt;. Using the ground truth labels, We will define the misclassification rate as the proportion of each digit not assigned to the same cluster that the majority of that digit is assigned to. The following code will allow us to calculate the misclassification rates for each digit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get.mode &amp;lt;- function(x) {
x.values &amp;lt;- unique(x)
totals &amp;lt;- tabulate(match(x, x.values))
return(c(x.values[which.max(totals)], max(totals)))
}
which.digit &amp;lt;- c()
for(i in 1:n){
which.digit &amp;lt;- c(which.digit, which.max(gamma[i, ]) )
}
digit.mapping &amp;lt;- matrix(0, 10, 2)
for(ii in 1:k){
indices &amp;lt;- which(which.digit == ii - 1)
digit.mapping[ii, ] &amp;lt;- get.mode(which.digit[indices]) }
ground.truth &amp;lt;- tabulate(match(which.digit, unique(which.digit)))
truth.prop &amp;lt;- digit.mapping[, 2] / ground.truth
mis.truth &amp;lt;- 1 - truth.prop
mis.class &amp;lt;- 1 - sum(digit.mapping[, 2]) / n&lt;/code&gt;&lt;/pre&gt;
Which gives us the following table: &lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;table style=&#34;width:62%;&#34;&gt;
&lt;caption&gt;MCR(misclassification rate)&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;13%&#34; /&gt;
&lt;col width=&#34;15%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Digit&lt;/th&gt;
&lt;th&gt;Cluster&lt;/th&gt;
&lt;th&gt;Quantity&lt;/th&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;th&gt;MCR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;0&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;1&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;87&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;161&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.46&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;1&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;7&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;99&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;162&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.39&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;2&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;3&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;105&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;159&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.34&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;3&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;6&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;105&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;159&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.34&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;4&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;8&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;138&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;161&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.14&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;5&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;10&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;76&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;159&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.52&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;6&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;4&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;116&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;161&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.28&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;7&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;2&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;134&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;159&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.15&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;8&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;5&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;118&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;155&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.24&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;p&gt;9&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;10&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;78&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;158&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;0.51&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This MCR table helps shed some light on inferences we made from the previous plots. Cluster 9, which most closely resembles a zero, was not the most common cluster for any digit. Cluster 10 was the most common cluster for both five and nine. The clusters with the lower misclassification rate tend to be the clusters with the most clearly defined mean plots in &lt;em&gt;Fig 2&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Test Post</title>
      <link>/2017/06/test-post/</link>
      <pubDate>Fri, 02 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/test-post/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Plain Markdown Post</title>
      <link>/2016/12/a-plain-markdown-post/</link>
      <pubDate>Fri, 30 Dec 2016 21:49:57 -0700</pubDate>
      
      <guid>/2016/12/a-plain-markdown-post/</guid>
      <description>&lt;p&gt;This is a post written in plain Markdown (&lt;code&gt;*.md&lt;/code&gt;) instead of R Markdown (&lt;code&gt;*.Rmd&lt;/code&gt;). The major differences are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (&lt;code&gt;```{r}&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;A plain Markdown post is rendered through &lt;a href=&#34;https://gohugo.io/overview/configuration/&#34;&gt;Blackfriday&lt;/a&gt;, and an R Markdown document is compiled by &lt;a href=&#34;http://rmarkdown.rstudio.com&#34;&gt;&lt;strong&gt;rmarkdown&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;http://pandoc.org&#34;&gt;Pandoc&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are many differences in syntax between Blackfriday&amp;rsquo;s Markdown and Pandoc&amp;rsquo;s Markdown. For example, you can write a task list with Blackfriday but not with Pandoc:&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Write an R package.&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Write a book.&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; &amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Profit!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, Blackfriday does not support LaTeX math and Pandoc does. I have added the MathJax support to this theme (&lt;a href=&#34;https://github.com/yihui/hugo-lithium-theme&#34;&gt;hugo-lithium-theme&lt;/a&gt;) but there is a caveat for plain Markdown posts: you have to include math expressions in a pair of backticks (inline: &lt;code&gt;`$ $`&lt;/code&gt;; display style: &lt;code&gt;`$$ $$`&lt;/code&gt;), e.g., &lt;code&gt;$S_n = \sum_{i=1}^n X_i$&lt;/code&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:This-is-because&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:This-is-because&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; For R Markdown posts, you do not need the backticks, because Pandoc can identify and process math expressions.&lt;/p&gt;

&lt;p&gt;When creating a new post, you have to decide whether the post format is Markdown or R Markdown, and this can be done via the &lt;code&gt;rmd&lt;/code&gt; argument of the function &lt;code&gt;blogdown::new_post()&lt;/code&gt;, e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::new_post(&amp;quot;Post Title&amp;quot;, rmd = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:This-is-because&#34;&gt;This is because we have to protect the math expressions from being interpreted as Markdown. You may not need the backticks if your math expression does not contain any special Markdown syntax such as underscores or asterisks, but it is always a safer choice to use backticks. When you happen to have a pair of literal dollar signs inside the same element, you can escape one dollar sign, e.g., &lt;code&gt;\$50 and $100&lt;/code&gt; renders &amp;ldquo;\$50 and $100&amp;rdquo;. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:This-is-because&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;This is a &amp;ldquo;hello world&amp;rdquo; example website for the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;strong&gt;blogdown&lt;/strong&gt;&lt;/a&gt; package. The theme was forked from &lt;a href=&#34;https://github.com/jrutheiser/hugo-lithium-theme&#34;&gt;@jrutheiser/hugo-lithium-theme&lt;/a&gt; and modified by &lt;a href=&#34;https://github.com/yihui/hugo-lithium-theme&#34;&gt;Yihui Xie&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/2015/07/hello-r-markdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/2015/07/hello-r-markdown/</guid>
      <description>&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lorem Ipsum</title>
      <link>/2015/01/lorem-ipsum/</link>
      <pubDate>Thu, 01 Jan 2015 13:09:13 -0600</pubDate>
      
      <guid>/2015/01/lorem-ipsum/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Lorem ipsum&lt;/strong&gt; dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore &lt;em&gt;magna aliqua&lt;/em&gt;. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>